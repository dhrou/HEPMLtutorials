{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZT7AoY8sKwp1"
      },
      "source": [
        "# Hands on : introduction to BDT on HEP dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eT7-MMpfrlHR"
      },
      "source": [
        "## Standard Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QdmaHjz-xCm-"
      },
      "outputs": [],
      "source": [
        "COLAB=True #if running on https://colab.research.google.com/notebooks/welcome.ipynb\n",
        "#COLAB=False #if running on local anaconda installation https://docs.anaconda.com/anaconda/install/\n",
        "             # need to download dataset by hand, see \"Load Event\" cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xy-_72FJKwp6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "#from IPython import display USEFUL ???\n",
        "from IPython.display import display, HTML\n",
        "%matplotlib inline\n",
        "import time\n",
        "pd.set_option('display.max_columns', 100) # to see more columns of df.head()\n",
        "np.random.seed(31415) # set the np random seed for the reproducibility\n",
        "#from IPython.core.interactiveshell import InteractiveShell\n",
        "#InteractiveShell.ast_node_interactivity = \"all\" # instead of \"last_expr\", allow all cell output to appear \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oF367-yfKwp9"
      },
      "outputs": [],
      "source": [
        "# Fix environment if necessary\n",
        "\n",
        "if COLAB:\n",
        "    !pip install xgboost --upgrade #  Colab has 0.9.0, not good enough\n",
        "    pass\n",
        "else:    \n",
        "    # install xgboost and lighgbm, two popular Boosted Decision Tree packages\n",
        "    # the following need to be done only once. To be commented out later\n",
        "    !pip install xgboost    \n",
        "    !pip install lightgbm \n",
        "    pass\n",
        "\n",
        "import xgboost\n",
        "print (xgboost.__version__) # Tested with 1.6.1, version above 1 is recommended.  \n",
        "import lightgbm\n",
        "print (lightgbm.__version__) # Tested with 2.2.3\n",
        "import sklearn\n",
        "print (sklearn.__version__) # Tested with 1.0.2\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IsChk6u-r7fk",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "if COLAB:\n",
        "    #### Reading files from Google Drive\n",
        "    # one need a google account to be identified\n",
        "    # select a google account, then cut and paste the long password in the pop up field\n",
        "    !pip install PyDrive\n",
        "    import os\n",
        "    from pydrive.auth import GoogleAuth\n",
        "    from pydrive.drive import GoogleDrive\n",
        "    from google.colab import auth\n",
        "    from oauth2client.client import GoogleCredentials\n",
        "    auth.authenticate_user()\n",
        "    gauth = GoogleAuth()\n",
        "    gauth.credentials = GoogleCredentials.get_application_default()\n",
        "    drive = GoogleDrive(gauth)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g14yIjbPKwqC"
      },
      "source": [
        "# Load events"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kl-4W4Ifs8EV"
      },
      "outputs": [],
      "source": [
        "if COLAB:\n",
        "\n",
        "    #attach dataset from google drive \n",
        "    download = drive.CreateFile({'id': '1nlXp7P-xq_jip4aPE0j0mnPhYnIOcBv4'})\n",
        "    download.GetContentFile(\"dataWW_d1_600k.csv.gz\")\n",
        "\n",
        "\n",
        "    datapath=\"\"\n",
        "\n",
        "\n",
        "    !ls -lrt\n",
        "else :\n",
        "    # make sure the file is available locally. \n",
        "    #Should be downloaded from https://drive.google.com/open?id=1nlXp7P-xq_jip4aPE0j0mnPhYnIOcBv4\n",
        "    !ls -lrt # what is in the local directory\n",
        "    datapath=\"/Users/rousseau/Google\\ Drive/GD_openData/dataWW_ATLAS_openData13TeV_filtered/\"\n",
        "\n",
        "    !ls -lrt {datapath} # what is in the data directory\n",
        "    datapath=os.path.abspath(datapath).replace(\"\\ \", \" \")  # try to normalise the path (annoyance with the space)\n",
        "    print (\"Will take data from : \",datapath)\n",
        "\n",
        "filename=os.path.join(datapath,\"dataWW_d1_600k.csv.gz\")\n",
        "#load data\n",
        "# data was created from ATLAS Open Data see doc\n",
        "# http://opendata.atlas.cern/release/2020/documentation/datasets/intro.html\n",
        "dfall = pd.read_csv(filename) \n",
        "\n",
        "#shuffle the events, already done but just to be safe\n",
        "dfall = dfall.sample(frac=1).reset_index(drop=True)\n",
        "from datetime import datetime\n",
        "print (\"now :\",datetime.now())\n",
        "print (\"File loaded with \",dfall.shape[0], \" events \")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uf9Gt8g8KwqF"
      },
      "source": [
        "At this point, it should tell you \"File Loaded with XXX events\". If not, it could not access the datafile, no point going further !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67cCYsYIKwqG"
      },
      "source": [
        "## Some data preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kaO2JM1hrlHr"
      },
      "outputs": [],
      "source": [
        "display(dfall.columns)\n",
        "dfall.mcWeight*=4 # arbitrary scale to have larger significance\n",
        "print (\"Df shape before selection :\", dfall.shape)\n",
        "# only keep events with exactly two leptons\n",
        "# WARNING only keep events with positive weight, as many tools choke on negative weights. \n",
        "# This is in wrong. One should at least keep the negative weights for testing.\n",
        "fulldata=dfall[ (dfall.lep_n==2) & (dfall.mcWeight > 0)]  \n",
        "\n",
        "print (\"Df shape after selection :\",fulldata.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nviyIMgerlH3"
      },
      "source": [
        "\n",
        "# DO NOT MODIFY ANYTHING ABOVE\n",
        "... and always rerun from this cell whenever you change something below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6e0Hlpv6rlH4",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "#hide label and weights in separate vectors\n",
        "#they are not real features\n",
        "#WARNING : there should be no selection nor shuffling later on ! (otherwise misalignement)\n",
        "\n",
        "target = fulldata[\"label\"]\n",
        "weights = fulldata[\"mcWeight\"]\n",
        "\n",
        "\n",
        "# for simplicity of the exercise only keep some features\n",
        "# this is actually making a deep copy from fulldata\n",
        "data=pd.DataFrame(fulldata, columns=[\"met_et\",\"met_phi\",\"lep_pt_0\",\"lep_pt_1\",'lep_phi_0', 'lep_phi_1'])\n",
        "#data=pd.DataFrame(fulldata, columns=[\"met_et\",\"met_phi\",\"lep_pt_0\",\"lep_pt_1\",'lep_eta_0', 'lep_eta_1', 'lep_phi_0', 'lep_phi_1','jet_n','jet_pt_0',\n",
        "#       'jet_pt_1', 'jet_eta_0', 'jet_eta_1', 'jet_phi_0', 'jet_phi_1']\n",
        "\n",
        "print (\"Df shape of dataset to be used :\",data.shape)\n",
        "display(data.head())\n",
        "display(target.head())\n",
        "display(weights.head())\n",
        "display(data.describe())\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Some data exploration"
      ],
      "metadata": {
        "id": "OOLb5S40D5wi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "IJAmtop6yd0a"
      },
      "outputs": [],
      "source": [
        "#data[data.met_et<1000]['met_et'].plot.hist(title='Missing Transverse Energy')\n",
        "data[data.lep_pt_0+data.lep_pt_1>1000]['met_et'].plot.hist(bins=np.linspace(0,400,100),title='Missing Transverse Energy for large lepton Pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "VfrUijp2yd0b"
      },
      "outputs": [],
      "source": [
        "fig=plt.figure()\n",
        "ax=data[target==0].plot.scatter(x='met_et', y='lep_pt_0',color=\"b\",label=\"B\")\n",
        "data[target==1].plot.scatter(x='met_et', y='lep_pt_0',color=\"r\",label=\"S\",ax=ax)\n",
        "#plt.legend(loc='best')\n",
        "#ax.set_xlabel('weight*1000')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "px76_qOlyd0c"
      },
      "outputs": [],
      "source": [
        "data[data.lep_pt_0+data.lep_pt_1>2000].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVHrJtodL2wo"
      },
      "source": [
        "## Examine the weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cFD4s616LS0F"
      },
      "outputs": [],
      "source": [
        "fig,ax=plt.subplots()\n",
        "#fig=plt.figure()\n",
        "\n",
        "bins=np.linspace(-1,3,101)\n",
        "plt.hist(weights[target==0]*1000,bins=bins,color='b',alpha=0.5,density=True,label='B ackground')\n",
        "plt.hist(weights[target==1]*1000,bins=bins,color='r',alpha=0.5,density=True,label='S ignal')\n",
        "plt.legend(loc='best')\n",
        "ax.set_xlabel('weight*1000')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrCZdWvqyd0e"
      },
      "source": [
        "## Some weight studies \n",
        "\n",
        "$s=\\sum w$ for signal dataset : predicted number of signal events (luminosity, cross section, efficiencies etc... already includded in the weights). Ditto for b background. \n",
        "\n",
        "\n",
        "Effective number of events fraction : $\\frac{N_{eff}}{N}= \\frac{1}{1+\\frac{Var(w)}{<w>^2}}$ . Example : if 0.2 it means the precision achieved with this dataset is the one which would be achieved with an unweighted dataset of 0.2 x N events (this is a rough estimate, only true for a simple counting)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YdpzI4-6MVIK"
      },
      "outputs": [],
      "source": [
        "label_n_weights=np.zeros(2)\n",
        "label_sum_weights=np.zeros(2)\n",
        "label_mean_weights=np.zeros(2)\n",
        "label_std_weights=np.zeros(2)\n",
        "label_neff_fraction=np.zeros(2)\n",
        "\n",
        "for i in range(2):\n",
        "  label_n_weights[i]=weights[target==i].size\n",
        "  label_mean_weights[i]=weights[target==i].mean()\n",
        "  label_std_weights[i]=weights[target==i].std()\n",
        "  label_sum_weights[i]=weights[target==i].sum()\n",
        "  label_neff_fraction[i]=1/(1+(label_std_weights[i]/label_mean_weights[i])**2)\n",
        "\n",
        "print (\"Weights quantities for background (target==0) and signal (target==1)\")\n",
        "print (\"Weights sum\",label_sum_weights)\n",
        "print (\"N events\",label_n_weights)\n",
        "print (\"Weights mean\",label_mean_weights)\n",
        "print (\"Weights std\",label_std_weights)\n",
        "print (\"Weights Neff fraction\",label_neff_fraction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F77Cx9LhKwqc"
      },
      "source": [
        "# Feature engineering\n",
        "To be switched on in a second iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "4oe_9ZEEKwqc"
      },
      "outputs": [],
      "source": [
        "if False: \n",
        "    data[\"lep_deltaphi\"]=np.abs(np.mod(data.lep_phi_1-data.lep_phi_0+3*np.pi,2*np.pi)-np.pi)\n",
        "    #data[\"lep_deltaphi\"]=data.lep_phi_1-data.lep_phi_0\n",
        "\n",
        "\n",
        "    print (data.shape)\n",
        "    display(data.head())\n",
        "   \n",
        "\n",
        "    \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDp3D2Fkyd0h"
      },
      "source": [
        "# Plot the features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2mf1bLVrlH7"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "\n",
        "ax=data[target==0].hist(weights=weights[target==0],figsize=(15,12),color='b',alpha=0.5,density=True,label=\"B\")\n",
        "ax=ax.flatten()[:data.shape[1]] # to avoid error if holes in the grid of plots (like if 7 or 8 features)\n",
        "data[target==1].hist(weights=weights[target==1],figsize=(15,12),color='r',alpha=0.5,density=True,ax=ax,label=\"S\")\n",
        "\n",
        "\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUMC85s4yd0i"
      },
      "source": [
        "### Features correlation matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "Kb-U3Ri5yd0i"
      },
      "outputs": [],
      "source": [
        "import seaborn as sn # seaborn for nice plot quicker\n",
        "print (\"Signal feature correlation matrix\")\n",
        "corrMatrix = data[target==1].corr()\n",
        "sn.heatmap(corrMatrix, annot=True)\n",
        "plt.show()\n",
        "\n",
        "print (\"Background feature correlation matrix\")\n",
        "corrMatrix = data[target==0].corr()\n",
        "sn.heatmap(corrMatrix, annot=True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kowHjX4rlIC"
      },
      "source": [
        "## Split dataset and transform the features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9j5hdrmrlID"
      },
      "outputs": [],
      "source": [
        "np.random.seed(31415) # set the random seed (used for the train/test splitting)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_size = 0.75 # fraction of sample used for training\n",
        "\n",
        "X_train, X_test, y_train, y_test, weights_train, weights_test = \\\n",
        "    train_test_split(data, target, weights, train_size=train_size)\n",
        "#reset index for dataseries, not needed for ndarray (X_train, X_test)\n",
        "y_train, y_test, weights_train, weights_test = \\\n",
        "    y_train.reset_index(drop=True),y_test.reset_index(drop=True), \\\n",
        "    weights_train.reset_index(drop=True), weights_test.reset_index(drop=True)\n",
        "\n",
        "print (X_train.shape)\n",
        "print (y_train.shape)\n",
        "print (weights_train.shape)\n",
        "print (X_test.shape)\n",
        "print (y_test.shape)\n",
        "print (weights_test.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# scale to mean 0 and variance 1\n",
        "# not really needed for BDT but we never know\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train) #calculate and apply the transformation to training dataset\n",
        "X_test = scaler.transform(X_test)  # apply to test dataset the transformation calculated the line above\n",
        "\n",
        "\n",
        "class_weights_train = (weights_train[y_train == 0].sum(), weights_train[y_train == 1].sum())\n",
        "\n",
        "for i in range(len(class_weights_train)): # loop on B then S target\n",
        "    #training dataset: equalize number of background and signal\n",
        "    weights_train[y_train == i] *= max(class_weights_train)/ class_weights_train[i] \n",
        "    #test dataset : increase test weight to compensate for sampling\n",
        "    weights_test[y_test == i] *= 1/(1-train_size) \n",
        "\n",
        "print (\"Weights have been normalised to a given number of proton collision\")    \n",
        "print (\"Orig : total weight sig\", weights[target == 1].sum())\n",
        "print (\"Orig : total weight bkg\", weights[target == 0].sum())\n",
        "\n",
        "\n",
        "print (\"Test : total weight sig\", weights_test[y_test == 1].sum())\n",
        "print (\"Test : total weight bkg\", weights_test[y_test == 0].sum())\n",
        "print (\"Train : total weight sig\", weights_train[y_train == 1].sum())\n",
        "print (\"Train : total weight bkg\", weights_train[y_train == 0].sum())\n",
        "\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxybCOi-rlIM"
      },
      "source": [
        "# Testing BDT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIsMSGl-Kwql"
      },
      "source": [
        "## Load significance function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "qubY3CMNKwql"
      },
      "outputs": [],
      "source": [
        "from math import sqrt\n",
        "from math import log\n",
        "def amsasimov(s,b): # asimov significance arXiv:1007.1727 eq. 97\n",
        "        if b<=0 or s<=0:\n",
        "            return 0\n",
        "        try:\n",
        "            return sqrt(2*((s+b)*log(1+float(s)/b)-s))\n",
        "        except ValueError:\n",
        "            print(1+float(s)/b)\n",
        "            print (2*((s+b)*log(1+float(s)/b)-s))\n",
        "        #return s/sqrt(s+b)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4DfF0ISrlIN"
      },
      "source": [
        "## XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqMCgvbkrlIN",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "np.random.seed(31415) # set the random seed\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import roc_auc_score # for binary classification if x > 0.5 -> 1 else -> 0\n",
        "# tree_method=\"hist\" is 10 times faster, however less robust against awkwards features (not a bad idea to double check without it)\n",
        "# can even try tree_method=\"gpu_hist\" if proper GPU installation\n",
        "# use_label_encoder and eval_metric to silence warning in 1.3.0\n",
        "xgb = XGBClassifier(tree_method=\"hist\",use_label_encoder=False,eval_metric='logloss')\n",
        "# HPO (==Hyper Parameter Optimization), check on the web https://xgboost.readthedocs.io/ for other parameters\n",
        "#xgb = XGBClassifier(tree_method=\"hist\",use_label_encoder=False,max_depth=10,n_estimators=100) \n",
        "\n",
        "\n",
        "\n",
        "starting_time = time.time()\n",
        "\n",
        "xgb.fit(X_train, y_train.values, sample_weight=weights_train.values) # note that XGB 1.3.X requires positive weight\n",
        "\n",
        "        \n",
        "training_time = time.time( ) - starting_time\n",
        "print(\"Training time:\",training_time)\n",
        "\n",
        "y_pred_xgb = xgb.predict_proba(X_test)[:,1]\n",
        "y_pred_xgb = y_pred_xgb.ravel()\n",
        "y_pred_train_xgb = xgb.predict_proba(X_train)[:,1].ravel()\n",
        "auc_test_xgb = roc_auc_score(y_true=y_test, y_score=y_pred_xgb,sample_weight=weights_test)\n",
        "print(\"auc test:\",auc_test_xgb)\n",
        "print (\"auc train:\",roc_auc_score(y_true=y_train.values, y_score=y_pred_train_xgb,sample_weight=weights_train))\n",
        "int_pred_test_sig_xgb = [weights_test[(y_test ==1) & (y_pred_xgb > th_cut)].sum() for th_cut in np.linspace(0,1,num=50)]\n",
        "int_pred_test_bkg_xgb = [weights_test[(y_test ==0) & (y_pred_xgb > th_cut)].sum() for th_cut in np.linspace(0,1,num=50)]\n",
        "\n",
        "vamsasimov_xgb = [amsasimov(sumsig,sumbkg) for (sumsig,sumbkg) in zip(int_pred_test_sig_xgb,int_pred_test_bkg_xgb)]\n",
        "significance_xgb = max(vamsasimov_xgb)\n",
        "Z = significance_xgb\n",
        "print(\"Z:\",Z)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_1SDqpwyd0m"
      },
      "outputs": [],
      "source": [
        "print(\"auc test\",roc_auc_score(y_true=y_test, y_score=y_pred_xgb,sample_weight=weights_test))\n",
        "print(\"auc test without weights\",roc_auc_score(y_true=y_test, y_score=y_pred_xgb))\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrlOjl-Dyd0m"
      },
      "source": [
        "# Hyper Parameter Optimisation\n",
        "Can be done by hand or with [random search](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) or [grid search](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "scrolled": true,
        "id": "0JyTm1fsyd0n"
      },
      "outputs": [],
      "source": [
        "#RandomSearchCV for advanced HPO \n",
        "import scipy.stats as stats\n",
        "if False:\n",
        "    from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "    # specify parameters and distributions to sample from\n",
        "    param_dist_XGB = {'max_depth': stats.randint(3, 12), # default 6\n",
        "                      'n_estimators': stats.randint(300, 800), #default 100\n",
        "                      'learning_rate': stats.uniform(0.1, 0.5)} #def 0.3 \n",
        "\n",
        "    # default CV is 5 fold, reduce to 2 for speed concern\n",
        "    gsearch = RandomizedSearchCV(estimator = XGBClassifier(tree_method=\"hist\",use_label_encoder=False,eval_metric='logloss'), \n",
        "                        param_distributions = param_dist_XGB, \n",
        "                        scoring='roc_auc',n_iter=10,cv=2)\n",
        "    gsearch.fit(X_train,y_train, sample_weight=weights_train)\n",
        "\n",
        "    print (\"Best parameters : \",gsearch.best_params_)\n",
        "    print (\"Best score (on train dataset CV) : \",gsearch.best_score_)\n",
        "\n",
        "\n",
        "    y_pred_gs = gsearch.predict_proba(X_test)[:,1]\n",
        "    print(\"... corresponding score on test dataset : \",roc_auc_score(y_true=y_test, y_score=y_pred_gs, sample_weight=weights_test))\n",
        "    dfsearch=pd.DataFrame.from_dict(gsearch.cv_results_)\n",
        "    display(dfsearch)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "O7xlfXkPyd0n"
      },
      "outputs": [],
      "source": [
        "if False: \n",
        "    dfsearch.plot.scatter(\"param_n_estimators\",\"mean_test_score\")\n",
        "    dfsearch.plot.scatter(\"param_max_depth\",\"mean_test_score\")\n",
        "    dfsearch.plot.scatter(\"param_learning_rate\",\"mean_test_score\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8kJjKBUyd0n"
      },
      "source": [
        "# Learning Curve\n",
        "This could be done with sklearn  [learning_curve](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.learning_curve.html).\n",
        "However (last time I checked) : it does not handle weights, it does not allow to control testing dataset size\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "dV6OalSjyd0o"
      },
      "outputs": [],
      "source": [
        "#example with sklearn (but without weights and with no fixed test dataset)\n",
        "#from sklearn.model_selection import learning_curve\n",
        "#train_sizes,train_scores,test_scores=learning_curve(\n",
        "#     XGBClassifier(tree_method=\"hist\",use_label_encoder=False,eval_metric='logloss',n_estimators=10),\n",
        "#     X_train,y_train,\n",
        "#     train_sizes=[0.01,0.05,0.1,0.2,0.5,0.75,1],                  \n",
        "#     scoring='roc_auc',cv=5)\n",
        "\n",
        "if False : \n",
        "    train_sizes=[0.01,0.05,0.1,0.2,0.5,0.75,1]\n",
        "    ntrains=[]\n",
        "    test_aucs=[]\n",
        "    train_aucs=[]\n",
        "    times=[]\n",
        "\n",
        "    for train_size in train_sizes:\n",
        "      ntrain=int(len(X_train)*train_size)\n",
        "      print(\"training with \",ntrain,\" events\")\n",
        "      ntrains+=[ntrain]\n",
        "      starting_time = time.time()\n",
        "\n",
        "      # train using the first ntrain event of the training dataset\n",
        "      xgb.fit(X_train[:ntrain,], y_train[:ntrain], sample_weight=weights_train[:ntrain])\n",
        "      training_time = time.time( ) - starting_time\n",
        "      times+=[training_time]\n",
        "\n",
        "      # score on test dataset (always the same)\n",
        "      y_pred_xgb=xgb.predict_proba(X_test)[:,1]\n",
        "      auc_test_xgb = roc_auc_score(y_true=y_test, y_score=y_pred_xgb,sample_weight=weights_test)\n",
        "      test_aucs+=[auc_test_xgb]\n",
        "\n",
        "      # score on the train dataset \n",
        "      y_train_xgb=xgb.predict_proba(X_train[:ntrain])[:,1]\n",
        "      auc_train_xgb = roc_auc_score(y_true=y_train[:ntrain], y_score=y_train_xgb,sample_weight=weights_train[:ntrain])\n",
        "      train_aucs+=[auc_train_xgb]\n",
        "\n",
        "    dflearning=pd.DataFrame({\"Ntraining\":ntrains,\n",
        "                             \"test_auc\":test_aucs,\n",
        "                             \"train_auc\":train_aucs,\n",
        "                             \"time\":times})\n",
        "    display(dflearning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "JWH3s-0Wyd0o"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "    dflearning.plot.scatter(\"Ntraining\",\"test_auc\")\n",
        "    # focus on the last point\n",
        "    dflearning[4:].plot.scatter(\"Ntraining\",\"test_auc\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmadcFRJrlIT"
      },
      "source": [
        "## LightGBM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9YmfuiwUrlIU"
      },
      "outputs": [],
      "source": [
        "np.random.seed(31415) # set the random seed\n",
        "\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import roc_auc_score # for binary classification if x > 0.5 -> 1 else -> 0\n",
        "#gbm = lgb.LGBMClassifier()\n",
        "gbm = lgb.LGBMClassifier()\n",
        "# gbm = lgb.LGBMClassifier(max_depth=12) # HPO, check on the web https://lightgbm.readthedocs.io/ for other parameters\n",
        "\n",
        "\n",
        "starting_time = time.time( )\n",
        "\n",
        "gbm.fit(X_train, y_train.values,sample_weight=weights_train.values)\n",
        "#gbm.fit(X_train, y_train.values) #ma\n",
        "\n",
        "\n",
        "training_time = time.time( ) - starting_time\n",
        "print(\"Training time:\",training_time)\n",
        "\n",
        "y_pred_gbm = gbm.predict_proba(X_test)[:,1]\n",
        "y_pred_gbm = y_pred_gbm.ravel()\n",
        "y_pred_train_gbm = gbm.predict_proba(X_train)[:,1].ravel()\n",
        "auc_test_gbm = roc_auc_score(y_true=y_test, y_score=y_pred_gbm,sample_weight=weights_test)\n",
        "print(\"auc test:\",auc_test_gbm)\n",
        "print (\"auc train:\",roc_auc_score(y_true=y_train.values, y_score=y_pred_train_gbm,sample_weight=weights_train))\n",
        "\n",
        "int_pred_test_sig_gbm = [weights_test[(y_test ==1) & (y_pred_gbm > th_cut)].sum() for th_cut in np.linspace(0,1,num=50)]\n",
        "int_pred_test_bkg_gbm = [weights_test[(y_test ==0) & (y_pred_gbm > th_cut)].sum() for th_cut in np.linspace(0,1,num=50)]\n",
        "\n",
        "vamsasimov_gbm = [amsasimov(sumsig,sumbkg) for (sumsig,sumbkg) in zip(int_pred_test_sig_gbm,int_pred_test_bkg_gbm)]\n",
        "significance_gbm = max(vamsasimov_gbm)\n",
        "Z = significance_gbm\n",
        "print(\"Z:\",Z)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7HnCErpyd0p"
      },
      "source": [
        "## SKLearn GBDT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eriizDZkyd0p"
      },
      "outputs": [],
      "source": [
        "from sklearn import ensemble\n",
        "\n",
        "# possible parameters, just take the default\n",
        "#original_params = {\n",
        "#    \"n_estimators\": 400,\n",
        "#    \"max_leaf_nodes\": 4,\n",
        "#    \"max_depth\": None,\n",
        "#    \"random_state\": 2,\n",
        "#    \"min_samples_split\": 5,\n",
        "#}\n",
        "\n",
        "skgb=ensemble.HistGradientBoostingClassifier()\n",
        "\n",
        "\n",
        "starting_time = time.time( )\n",
        "\n",
        "skgb.fit(X_train, y_train.values,sample_weight=weights_train.values)\n",
        "\n",
        "\n",
        "training_time = time.time( ) - starting_time\n",
        "print(\"Training time:\",training_time)\n",
        "\n",
        "y_pred_skgb = skgb.predict_proba(X_test)[:,1]\n",
        "y_pred_skgb = y_pred_skgb.ravel()\n",
        "y_pred_train_skgb = skgb.predict_proba(X_train)[:,1].ravel()\n",
        "auc_test_skgb = roc_auc_score(y_true=y_test, y_score=y_pred_skgb,sample_weight=weights_test)\n",
        "print(\"auc test:\",auc_test_skgb)\n",
        "print (\"auc train:\",roc_auc_score(y_true=y_train.values, y_score=y_pred_train_skgb,sample_weight=weights_train))\n",
        "\n",
        "int_pred_test_sig_skgb = [weights_test[(y_test ==1) & (y_pred_skgb > th_cut)].sum() for th_cut in np.linspace(0,1,num=50)]\n",
        "int_pred_test_bkg_skgb = [weights_test[(y_test ==0) & (y_pred_skgb > th_cut)].sum() for th_cut in np.linspace(0,1,num=50)]\n",
        "\n",
        "vamsasimov_skgb = [amsasimov(sumsig,sumbkg) for (sumsig,sumbkg) in zip(int_pred_test_sig_skgb,int_pred_test_bkg_skgb)]\n",
        "significance_skgb = max(vamsasimov_skgb)\n",
        "Z = significance_skgb\n",
        "print(\"Z:\",Z)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pSxtcTGyd0q"
      },
      "outputs": [],
      "source": [
        "print('Best significance found are:')\n",
        "print('XGBoost : ', significance_xgb)\n",
        "print('LightGBM: ', significance_gbm)\n",
        "print('sklearn: ', significance_skgb)\n",
        "\n",
        "\n",
        "print('Best auc test found are:')\n",
        "print('XGBoost: ', roc_auc_score(y_true=y_test.values, y_score=y_pred_xgb,sample_weight=weights_test)) \n",
        "print('LightGBM: ', roc_auc_score(y_true=y_test.values, y_score=y_pred_gbm,sample_weight=weights_test))\n",
        "print('sklearn: ', roc_auc_score(y_true=y_test.values, y_score=y_pred_skgb,sample_weight=weights_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-LB9cbErlIb"
      },
      "source": [
        "# Some nice plots "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Ay8QTiKPKwqz"
      },
      "outputs": [],
      "source": [
        "# some utilities\n",
        "from math import sqrt\n",
        "from math import log\n",
        "\n",
        "# Plot score for signal and background, comparing training and testing\n",
        "def compare_train_test(y_pred_train, y_train, y_pred, y_test, high_low=(0,1), \n",
        "                       bins=30,xlabel=\"\", ylabel=\"Arbitrary units\", title=\"\", \n",
        "                       weights_train=np.array([]), weights_test=np.array([]),\n",
        "                       density=True):\n",
        "    if weights_train.size != 0:\n",
        "        weights_train_signal = weights_train[y_train == 1]\n",
        "        weights_train_background = weights_train[y_train == 0]\n",
        "    else:\n",
        "        weights_train_signal = None\n",
        "        weights_train_background = None\n",
        "    plt.hist(y_pred_train[y_train == 1],\n",
        "                 color='r', alpha=0.5, range=high_low, bins=bins,\n",
        "                 histtype='stepfilled', density=density,\n",
        "                 label='S (train)', weights=weights_train_signal) # alpha is transparancy\n",
        "    plt.hist(y_pred_train[y_train == 0],\n",
        "                 color='b', alpha=0.5, range=high_low, bins=bins,\n",
        "                 histtype='stepfilled', density=density,\n",
        "                 label='B (train)', weights=weights_train_background)\n",
        "\n",
        "    if weights_test.size != 0:\n",
        "        weights_test_signal = weights_test[y_test == 1]\n",
        "        weights_test_background = weights_test[y_test == 0]\n",
        "    else:\n",
        "        weights_test_signal = None\n",
        "        weights_test_background = None\n",
        "    hist, bins = np.histogram(y_pred[y_test == 1],\n",
        "                                  bins=bins, range=high_low, density=density, weights=weights_test_signal)\n",
        "    scale = len(y_pred[y_test == 1]) / sum(hist)\n",
        "    err = np.sqrt(hist * scale) / scale\n",
        "\n",
        "    center = (bins[:-1] + bins[1:]) / 2\n",
        "    plt.errorbar(center, hist, yerr=err, fmt='o', c='r', label='S (test)')\n",
        "\n",
        "    hist, bins = np.histogram(y_pred[y_test == 0],\n",
        "                                  bins=bins, range=high_low, density=density, weights=weights_test_background)\n",
        "    scale = len(y_pred[y_test == 0]) / sum(hist)\n",
        "    err = np.sqrt(hist * scale) / scale\n",
        "\n",
        "    center = (bins[:-1] + bins[1:]) / 2\n",
        "    plt.errorbar(center, hist, yerr=err, fmt='o', c='b', label='B (test)')\n",
        "    plt.title(title)\n",
        "    plt.xlabel(xlabel)\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.legend(loc='best')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Score plot"
      ],
      "metadata": {
        "id": "VBbFEyJOEn-w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kEQ342s-rlIe"
      },
      "outputs": [],
      "source": [
        "\n",
        "compare_train_test(y_pred_train_xgb, y_train, y_pred_xgb, y_test, \n",
        "                   xlabel=\"XGboost score\", title=\"XGboost\", \n",
        "                   weights_train=weights_train.values, weights_test=weights_test.values)\n",
        "plt.savefig(\"Score_BDT_XGBoost_Hist.pdf\")\n",
        "plt.show()\n",
        "compare_train_test(y_pred_train_gbm, y_train, y_pred_gbm, y_test, \n",
        "                   xlabel=\"LightGBM score\", title=\"LightGBM\", \n",
        "                   weights_train=weights_train.values, weights_test=weights_test.values)\n",
        "plt.savefig(\"Score_BDT_LightGBM.pdf\")\n",
        "plt.show()\n",
        "compare_train_test(y_pred_train_skgb, y_train, y_pred_skgb, y_test, \n",
        "                   xlabel=\"sklearn score\", title=\"sklearn\", \n",
        "                   weights_train=weights_train.values, weights_test=weights_test.values)\n",
        "plt.savefig(\"Score_BDT_sklearn.pdf\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPv67MTLyd0s"
      },
      "source": [
        "## Score plot without renormalising"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "visTZB8eyd0t"
      },
      "outputs": [],
      "source": [
        "compare_train_test(y_pred_train_xgb, y_train, y_pred_xgb, y_test, \n",
        "                   xlabel=\"XGboost score\", ylabel=\"Expected number of events\", title=\"XGboost\", \n",
        "                   weights_train=weights_train.values, weights_test=weights_test.values, \n",
        "                   density=False)\n",
        "plt.savefig(\"Score_BDT_XGBoost_Hist.pdf\")\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ROC curve"
      ],
      "metadata": {
        "id": "zxGjEcej8PIi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tUKIWu2orlIg"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve\n",
        "lw = 2\n",
        "\n",
        "fpr_xgb,tpr_xgb,_ = roc_curve(y_true=y_test, y_score=y_pred_xgb,sample_weight=weights_test.values)\n",
        "fpr_gbm,tpr_gbm,_ = roc_curve(y_true=y_test, y_score=y_pred_gbm,sample_weight=weights_test.values)\n",
        "fpr_skgb,tpr_skgb,_ = roc_curve(y_true=y_test, y_score=y_pred_skgb,sample_weight=weights_test.values)\n",
        "\n",
        "\n",
        "plt.plot(fpr_xgb, tpr_xgb, color='darkgreen',lw=lw, label='XGBoost (AUC  = {})'.format(np.round(auc_test_xgb,decimals=2)))\n",
        "plt.plot(fpr_skgb, tpr_skgb, color='darkblue',lw=lw, label='sklearn (AUC  = {})'.format(np.round(auc_test_skgb,decimals=2)))\n",
        "plt.plot(fpr_gbm, tpr_gbm, color='darkorange',lw=lw, label='LightGBM (AUC  = {})'.format(np.round(auc_test_gbm,decimals=2)))\n",
        "\n",
        "\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.legend(loc=\"lower right\")\n",
        "#import os\n",
        "#new_dir = \"Plots/Comparing\" \n",
        "#if not os.path.isdir(new_dir):\n",
        "#    os.mkdir(new_dir)\n",
        "plt.savefig(\"ROC_comparing.pdf\")\n",
        "plt.show() # blue line = random classification -> maximize true positive rate while miniize false positive rate"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Significance curve"
      ],
      "metadata": {
        "id": "ziQzdapn8Tnk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGF3k0KJrlIi"
      },
      "outputs": [],
      "source": [
        "plt.plot(np.linspace(0,1,num=50),vamsasimov_xgb, label='XGBoost (Z = {})'.format(np.round(significance_xgb,decimals=2)))\n",
        "plt.plot(np.linspace(0,1,num=50),vamsasimov_skgb, label='sklearn (Z = {})'.format(np.round(significance_skgb,decimals=2)))\n",
        "plt.plot(np.linspace(0,1,num=50),vamsasimov_gbm, label='LightGBM (Z = {})'.format(np.round(significance_gbm,decimals=2)))\n",
        "\n",
        "\n",
        "\n",
        "plt.title(\"BDT Significance\")\n",
        "plt.xlabel(\"Threshold\")\n",
        "plt.ylabel(\"Significance\")\n",
        "plt.legend()\n",
        "plt.savefig(\"Significance_comparing.pdf\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJI9aimTyd0v"
      },
      "source": [
        "# Feature importance\n",
        "Feature importance allows to display the importance of each feature without rerunnning the training. It is obtained from internal algorithm quantities, like number of time a feature is used to definea leaf. Magnitude is arbitrary. It can be used as a not very reliable indication of which feature is the most discriminant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QN2h9y5JrlIm"
      },
      "outputs": [],
      "source": [
        "plt.bar(data.columns.values, xgb.feature_importances_)\n",
        "plt.xticks(rotation=90)\n",
        "plt.title(\"Feature importances XGBoost Hist\")\n",
        "#plt.savefig(new_dir + \"/VarImp_BDT_XGBoost_Hist.pdf\",bbox_inches='tight')\n",
        "plt.show()\n",
        "plt.bar(data.columns.values, gbm.feature_importances_)\n",
        "plt.xticks(rotation=90)\n",
        "plt.title(\"Feature importances LightGBM\")\n",
        "#plt.savefig(new_dir + \"/VarImp_BDT_LightGBM.pdf\",bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7WxH3bDrlIp"
      },
      "source": [
        "# Permutation importance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55ZOBmxxyd0w"
      },
      "source": [
        "A better way to show the importance of each feature is Permutation Importance, where each feature in turn is replaced by an instance of an other event (effectively switching it off by randomising).\n",
        "In particular it allows to : \n",
        "   * display directly the loss in whatever criteria (ROC auc, asimov significance) when the feature is switched off\n",
        "   * display the feature importance for a specific subset (for example the most signal like)\n",
        "   * it can even display which feature has the larges impact on systematics\n",
        "\n",
        "\n",
        "However, report can be misleading in case of highly correlated variables. \n",
        "   \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if True:\n",
        "  from sklearn.inspection import permutation_importance\n",
        "  r = permutation_importance(xgb, X_test, y_test,sample_weight=weights_test,\n",
        "                           scoring='roc_auc',n_repeats=1,n_jobs=-1,\n",
        "                            random_state=0)\n",
        "  plt.bar(data.columns,r.importances.mean(axis=1).T,)\n",
        "\n",
        "  plt.xlabel('features')\n",
        "  plt.ylabel('impact on auc')\n",
        "  plt.title('Permutation Importance XGBoost')\n",
        "\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "CeMWnl2v00JH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BiGDmMS-yd0x"
      },
      "source": [
        "##  Model serialisation\n",
        "It is useful to be able to save a model in order to apply it without retraining. There are many ways to do it. One is to save the whole python object with joblib (beware this is not safe if the software evolves).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "Nlj3NPpkyd0y"
      },
      "outputs": [],
      "source": [
        "#WARNING : StandardScaler has not been saved\n",
        "# one can look into sklearn pipeline\n",
        "if False:\n",
        "    import joblib\n",
        "    myxgb = XGBClassifier(tree_method=\"hist\",use_label_encoder=False,eval_metric='logloss',n_estimators=5)\n",
        "    myxgb.fit(X_train, y_train, sample_weight=weights_train)\n",
        "\n",
        "    auc_test_xgb = roc_auc_score(y_true=y_test, y_score=myxgb.predict_proba(X_test)[:,1],sample_weight=weights_test)\n",
        " \n",
        "    # save model\n",
        "    myxgb.save_model(\"XGBoost.json\")\n",
        "\n",
        "\n",
        "    # save python object\n",
        "    joblib.dump(myxgb, \"myxgb.dat\")\n",
        "\n",
        "    print (\"myxgb score\",auc_test_xgb)\n",
        "\n",
        "    del myxgb # delete xgb object\n",
        "\n",
        "    # reload model\n",
        "    myxgb_reloaded_from_model =XGBClassifier()\n",
        "    myxgb_reloaded_from_model.load_model(\"XGBoost.json\")\n",
        "    print (\"myxgb reloaded from model\",\n",
        "           roc_auc_score(y_true=y_test, \n",
        "                         y_score=myxgb_reloaded_from_model.predict_proba(X_test)[:,1],sample_weight=weights_test)\n",
        "          )\n",
        "\n",
        "\n",
        "    # reload object\n",
        "    myxgb_reloaded_from_joblib=joblib.load(\"myxgb.dat\")\n",
        "    print (\"myxgb reloaded from object\",\n",
        "           roc_auc_score(y_true=y_test, \n",
        "                         y_score=myxgb_reloaded_from_joblib.predict_proba(X_test)[:,1],sample_weight=weights_test)\n",
        "          )\n",
        "\n",
        "    !cat XGBoost.json\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "print(datetime.datetime.now())"
      ],
      "metadata": {
        "id": "5cT0LcABFXN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1mXjZdbMFarz"
      },
      "execution_count": 35,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "HEPML_HandsOn_BDT.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}